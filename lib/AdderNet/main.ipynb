{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bf74675f",
      "metadata": {
        "id": "bf74675f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from model import AdderNet, AdderNet2_0\n",
        "from Train import model_training\n",
        "from quantization_encoder import Quant\n",
        "from quantization_decoding import Quant_decode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bf2a9133",
      "metadata": {
        "id": "bf2a9133"
      },
      "outputs": [],
      "source": [
        "# Hyper parameters\n",
        "epoch_losses = []\n",
        "train_loss_list = []\n",
        "lr = 0.001\n",
        "epochs = 200\n",
        "train_batch_size = 256\n",
        "test_batch_size = 128\n",
        "in_channels = 3\n",
        "out_channels = 16\n",
        "kernel_size = 3\n",
        "\n",
        "# Model device assignment\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = AdderNet().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b140399",
      "metadata": {
        "id": "4b140399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad5e25b-cf3e-4718-b096-de60a6b092cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|â–‰         | 15.9M/170M [00:54<08:25, 306kB/s]"
          ]
        }
      ],
      "source": [
        "# Define transformation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                        (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Test transform (no augmentation)\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                        (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# Load the training dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=train_transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=train_batch_size,\n",
        "                                          shuffle=True, num_workers=8)\n",
        "\n",
        "# Load the test dataset\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=test_transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=test_batch_size,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "# Define the class names for reference\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd80804",
      "metadata": {
        "id": "6fd80804"
      },
      "outputs": [],
      "source": [
        "# Training AdderNet model on Cifar10\n",
        "train_model = model_training(model, lr, trainloader, epochs)\n",
        "training_loss = train_model.forward(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7aea478",
      "metadata": {
        "id": "f7aea478"
      },
      "outputs": [],
      "source": [
        "# Visualization of training results\n",
        "plt.plot(training_loss, linewidth=2, c='r')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss of Addernet Model\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd326a6",
      "metadata": {
        "id": "2bd326a6"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    number_correct = 0\n",
        "    samples = 0\n",
        "    for images, labels in testloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        _, prediction = torch.max(outputs, 1)\n",
        "        samples += labels.size(0)\n",
        "        number_correct += (prediction == labels).sum().item()\n",
        "\n",
        "accuracy = (number_correct / samples) * 100.0\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7889eafc",
      "metadata": {
        "id": "7889eafc"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), 'AdderNet_model.pth')\n",
        "print(\"Model saved as 'AdderNet_model.pth'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aeace32",
      "metadata": {
        "id": "8aeace32"
      },
      "outputs": [],
      "source": [
        "# Editing .pth state dictionary\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_state_dict = torch.load('AdderNet_model.pth', map_location=device)\n",
        "fixed_state_dict = {}\n",
        "\n",
        "def remap_key(key: str) -> str:\n",
        "    \"\"\"Map original checkpoint keys to the correct AdderNet naming.\"\"\"\n",
        "    new_key = key.replace('module.', '')\n",
        "\n",
        "    # conv and batchnorm\n",
        "    if new_key.startswith('conv1.') or new_key.startswith('bn1.') or new_key.startswith('fc.') or new_key.startswith('bn2.'):\n",
        "        return new_key\n",
        "\n",
        "    # Process residual layers\n",
        "    for layer_num in [1, 2, 3]:\n",
        "        prefix = f'layer{layer_num}.'\n",
        "        if new_key.startswith(prefix):\n",
        "            rest = new_key[len(prefix):]  # everything after 'layerX.'\n",
        "\n",
        "            # If next is block index\n",
        "            if rest[0].isdigit():\n",
        "                block_num = rest[0]\n",
        "                rest_after_block = rest[2:]\n",
        "\n",
        "                # Handle downsample case\n",
        "                if rest_after_block.startswith('downsample.'):\n",
        "                    ds_rest = rest_after_block[len('downsample.'):]\n",
        "                    if ds_rest.startswith('0.'):\n",
        "                        return f'layer{layer_num}.downsample_adder.{ds_rest[2:]}'\n",
        "                    elif ds_rest.startswith('1.'):\n",
        "                        return f'layer{layer_num}.downsample_bn.{ds_rest[2:]}'\n",
        "\n",
        "                # Otherwise: normal residual block conv/bn\n",
        "                rest_after_block = rest_after_block.replace('conv1.', 'adder1.')\n",
        "                rest_after_block = rest_after_block.replace('conv2.', 'adder2.')\n",
        "                return f'layer{layer_num}.blocks.{block_num}.{rest_after_block}'\n",
        "\n",
        "    # If nothing matches, just return\n",
        "    return new_key\n",
        "\n",
        "# Apply the mapping\n",
        "for key, value in model_state_dict.items():\n",
        "    fixed_key = remap_key(key)\n",
        "    fixed_state_dict[fixed_key] = value\n",
        "\n",
        "params = list(fixed_state_dict.items())\n",
        "n_params = len(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f474ef05",
      "metadata": {
        "id": "f474ef05"
      },
      "outputs": [],
      "source": [
        "n_cols = 16  # 16 columns\n",
        "n_rows = (n_params + n_cols - 1) // n_cols  # 8 rows for 128 params\n",
        "\n",
        "# Create subplots with larger figure\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(50, 25))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each parameter\n",
        "for i, (name, tensor) in enumerate(params):\n",
        "    values = tensor.cpu().numpy().flatten()\n",
        "    axes[i].hist(values, bins=50)\n",
        "    axes[i].set_title(name)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a021d0c",
      "metadata": {
        "id": "1a021d0c"
      },
      "outputs": [],
      "source": [
        "filtered_params = []\n",
        "for name, tensor in params:\n",
        "    # Include if name contains: weight, bias, adder\n",
        "    if any(keyword in name.lower() for keyword in ['weight', 'bias', 'adder']):\n",
        "        filtered_params.append((name, tensor))\n",
        "\n",
        "n_params = len(filtered_params)\n",
        "\n",
        "n_cols = 5  # 5 columns\n",
        "n_rows = (n_params + n_cols - 1) // n_cols  # 13 rows for 65 params\n",
        "\n",
        "# Create subplots with larger figure\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(50, 25))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each parameter\n",
        "for i, (name, tensor) in enumerate(filtered_params):\n",
        "    values = tensor.cpu().numpy().flatten()\n",
        "    axes[i].hist(values, bins=50)\n",
        "    axes[i].set_title(name)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4db55afd",
      "metadata": {
        "id": "4db55afd"
      },
      "outputs": [],
      "source": [
        "# Quantization of weight tensors for AdderNet\n",
        "quant_acc = []\n",
        "quant_error = []\n",
        "bit_array = [16, 14, 12, 10, 8, 5, 6, 4, 3]\n",
        "activations_dict = {}\n",
        "\n",
        "for i, bits in enumerate(bit_array):\n",
        "  activations_dict[bits] = {\n",
        "    'input_activation': None,\n",
        "    'prelayer_activation': None,\n",
        "    'layer1_activation': None,\n",
        "    'layer2_activation': None,\n",
        "    'layer3_activation': None\n",
        "  }\n",
        "  quant_dict = {}\n",
        "  max_error = 0.0\n",
        "\n",
        "  for name, weight_tensor in params:\n",
        "    quantized_tensor, scale = Quant.symmetric_quantization(weight_tensor, bits=bits)\n",
        "    fp32_tensor, error = Quant_decode.dequant(scale, quantized_tensor, weight_tensor)\n",
        "    quant_dict[name] = fp32_tensor.to(device)\n",
        "    max_error = max(max_error, error)\n",
        "\n",
        "  quant_model = AdderNet().to(device)\n",
        "  quant_model.load_manual_weights(quant_dict)\n",
        "  quant_model.eval()\n",
        "\n",
        "  print(f\"Testing {bits}-bit quantization\")\n",
        "  with torch.no_grad():\n",
        "      number_correct = 0\n",
        "      samples = 0\n",
        "      first_batch = True\n",
        "      for images, labels in testloader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = quant_model(images, save_activations=True)\n",
        "\n",
        "          _, prediction = torch.max(outputs, 1)\n",
        "          samples += labels.size(0)\n",
        "          number_correct += (prediction == labels).sum().item()\n",
        "\n",
        "          if first_batch:\n",
        "            for name, activation in quant_model.activations.items():\n",
        "              activations_dict[bits][name] = activation.cpu().clone()\n",
        "            first_batch = False\n",
        "\n",
        "  del quant_model\n",
        "  torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "\n",
        "  accuracy = (number_correct / samples) * 100.0\n",
        "  quant_acc.append(accuracy)\n",
        "  print(f\"  Accuracy: {accuracy:.2f}%\")\n",
        "  quant_error.append(max_error)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab447ab",
      "metadata": {
        "id": "6ab447ab"
      },
      "outputs": [],
      "source": [
        "# AOQ of weight tensors for AdderNet2.0\n",
        "quant_acc_2_0 = []\n",
        "bit_array = [16, 14, 12, 10, 8, 6, 5, 4, 3]\n",
        "activations_dict2_0 = {}\n",
        "clipped_weight_dict = {}\n",
        "clipped_bias_dict = {}\n",
        "\n",
        "for i, bits in enumerate(bit_array):\n",
        "  activations_dict2_0[bits] = {\n",
        "    'input_activation_2.0': None,\n",
        "    'prelayer_activation_2.0': None,\n",
        "    'layer1_activation_2.0': None,\n",
        "    'layer2_activation_2.0': None,\n",
        "    'layer3_activation_2.0': None\n",
        "  }\n",
        "\n",
        "  quantized_state_dict = {k: v.clone() for k, v in fixed_state_dict.items()}\n",
        "\n",
        "  max_val = 2.5\n",
        "  Max_A = 2**(bits-1) - 1      # Maximum value: 127 for bits=8\n",
        "  Max_B = -1 * 2**(bits-1)     # Minimum value: -128 for bits=8\n",
        "  delta = max_val / (2**(bits-1) - 1)\n",
        "\n",
        "  print(f\"\\nProcessing {bits}-bit quantization with FBR. Delta Val: {delta}\")\n",
        "\n",
        "  x_tensor = quantized_state_dict['bn1.weight']\n",
        "  xq_tensor = x_tensor / delta\n",
        "  quantized_state_dict['bn1.weight'] = xq_tensor\n",
        "\n",
        "  x_tensor = quantized_state_dict['bn1.bias']\n",
        "  xq_tensor = x_tensor / delta\n",
        "  quantized_state_dict['bn1.bias'] = xq_tensor\n",
        "\n",
        "  clipped_bias_dict[bits] = {}\n",
        "  clipped_weight_dict[bits] = {}\n",
        "\n",
        "  for name in quantized_state_dict.keys():\n",
        "      if name.startswith('layer'):\n",
        "          if name.endswith('adder'):\n",
        "              w_tensor = quantized_state_dict[name]\n",
        "              wq = torch.round(w_tensor/delta)\n",
        "              wq_clamp = torch.clamp(wq, max=Max_A, min=Max_B)\n",
        "              quantized_state_dict[name] = wq_clamp\n",
        "              bias_tensor = (wq - wq_clamp).abs()\n",
        "              bias_sum = torch.sum(bias_tensor, dim=(1,2,3))\n",
        "              clipped_weight_dict[bits][name] = bias_sum.cpu().clone()\n",
        "              clipped_bias_dict[bits][name.replace('adder', 'bn')] = bias_sum.cpu().clone()\n",
        "          if name.endswith('running_mean'):\n",
        "              m_tensor = quantized_state_dict[name]\n",
        "              mq = torch.round(m_tensor/delta)\n",
        "              quantized_state_dict[name] = mq + bias_sum\n",
        "          if name.endswith('bias'):\n",
        "              x_tensor=quantized_state_dict[name]\n",
        "              xq_tensor = x_tensor/delta\n",
        "              quantized_state_dict[name] = xq_tensor\n",
        "\n",
        "  quantized_state_dict['fc.weight']=quantized_state_dict['fc.weight']*delta\n",
        "\n",
        "  quant_model = AdderNet2_0(num_classes=10).to(device)\n",
        "  quant_model.load_manual_weights(quantized_state_dict)\n",
        "  quant_model.eval()\n",
        "\n",
        "  print(f\"Testing {bits}-bit quantization AdderNet2.0\")\n",
        "  with torch.no_grad():\n",
        "      number_correct = 0\n",
        "      samples = 0\n",
        "      first_batch = True\n",
        "      for images, labels in testloader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = quant_model(images, save_activations=True)\n",
        "\n",
        "          _, prediction = torch.max(outputs, 1)\n",
        "          samples += labels.size(0)\n",
        "          number_correct += (prediction == labels).sum().item()\n",
        "\n",
        "          if first_batch:\n",
        "            for name, activation in quant_model.activations.items():\n",
        "              activations_dict2_0[bits][name] = activation.cpu().clone()\n",
        "            first_batch = False\n",
        "\n",
        "  accuracy = (number_correct / samples) * 100.0\n",
        "  print(f\"  Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "  del quant_model\n",
        "  torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "  quant_acc_2_0.append(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AOQ of unsigned weight tensors for AdderNet2.0\n",
        "quant_acc_2_0_U = []\n",
        "bit_array = [16, 14, 12, 10, 8, 6, 5, 4, 3]\n",
        "activations_dict2_0_U = {}\n",
        "clipped_bias_dict_U = {}\n",
        "clipped_weight_dict_U = {}\n",
        "\n",
        "for i, bits in enumerate(bit_array):\n",
        "  activations_dict2_0_U[bits] = {\n",
        "    'input_activation_2.0': None,\n",
        "    'prelayer_activation_2.0': None,\n",
        "    'layer1_activation_2.0': None,\n",
        "    'layer2_activation_2.0': None,\n",
        "    'layer3_activation_2.0': None\n",
        "  }\n",
        "\n",
        "  quantized_state_dict = {k: v.clone() for k, v in fixed_state_dict.items()}\n",
        "\n",
        "  max_val = 2.5\n",
        "  Max_A = 2**(bits) - 1      # Maximum value: 127 for bits=8\n",
        "  Max_B = 0                    # Minimum value: 0 for unsigned\n",
        "  delta = max_val / (Max_A)\n",
        "\n",
        "  print(f\"\\nProcessing {bits}-bit quantization with FBR. Delta Val: {delta}\")\n",
        "\n",
        "  x_tensor = quantized_state_dict['bn1.weight']\n",
        "  xq_tensor = x_tensor / delta\n",
        "  quantized_state_dict['bn1.weight'] = xq_tensor\n",
        "\n",
        "  x_tensor = quantized_state_dict['bn1.bias']\n",
        "  xq_tensor = x_tensor / delta\n",
        "  quantized_state_dict['bn1.bias'] = xq_tensor\n",
        "\n",
        "  clipped_bias_dict_U[bits] = {}\n",
        "  clipped_weight_dict_U[bits] = {}\n",
        "\n",
        "  for name in quantized_state_dict.keys():\n",
        "      if name.startswith('layer'):\n",
        "          if name.endswith('adder'):\n",
        "              w_tensor = quantized_state_dict[name]\n",
        "              wq = torch.round(w_tensor/delta)\n",
        "              wq_clamp = torch.clamp(wq, max=Max_A, min=Max_B)\n",
        "              quantized_state_dict[name] = wq_clamp\n",
        "              bias_tensor = (wq - wq_clamp).abs()\n",
        "              bias_sum = torch.sum(bias_tensor, dim=(1,2,3))\n",
        "              clipped_weight_dict_U[bits][name] = bias_sum.cpu().clone()\n",
        "              clipped_bias_dict_U[bits][name.replace('adder', 'bn')] = bias_sum.cpu().clone()\n",
        "          if name.endswith('running_mean'):\n",
        "              m_tensor = quantized_state_dict[name]\n",
        "              mq = torch.round(m_tensor/delta)\n",
        "              quantized_state_dict[name] = mq + bias_sum\n",
        "          if name.endswith('bias'):\n",
        "              x_tensor=quantized_state_dict[name]\n",
        "              xq_tensor = x_tensor/delta\n",
        "              quantized_state_dict[name] = xq_tensor\n",
        "\n",
        "  quantized_state_dict['fc.weight']=quantized_state_dict['fc.weight']*delta\n",
        "\n",
        "  quant_model = AdderNet2_0(num_classes=10).to(device)\n",
        "  quant_model.load_manual_weights(quantized_state_dict)\n",
        "  quant_model.eval()\n",
        "\n",
        "  print(f\"Testing {bits}-bit quantization AdderNet2.0\")\n",
        "  with torch.no_grad():\n",
        "      number_correct = 0\n",
        "      samples = 0\n",
        "      first_batch = True\n",
        "      for images, labels in testloader:\n",
        "          images = images.to(device)\n",
        "          labels = labels.to(device)\n",
        "          outputs = quant_model(images, save_activations=True)\n",
        "\n",
        "          _, prediction = torch.max(outputs, 1)\n",
        "          samples += labels.size(0)\n",
        "          number_correct += (prediction == labels).sum().item()\n",
        "\n",
        "          if first_batch:\n",
        "            for name, activation in quant_model.activations.items():\n",
        "              activations_dict2_0_U[bits][name] = activation.cpu().clone() # Fixed: Changed to activations_dict2_0_U\n",
        "            first_batch = False\n",
        "\n",
        "  accuracy = (number_correct / samples) * 100.0\n",
        "  print(f\"  Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "  del quant_model\n",
        "  torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
        "  quant_acc_2_0_U.append(accuracy)\n"
      ],
      "metadata": {
        "id": "2Q76vvKg0cGA"
      },
      "id": "2Q76vvKg0cGA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for bits in clipped_bias_dict.keys():\n",
        "    # Plot bias distribution for this bit width\n",
        "    all_biases = torch.cat([v.flatten() for v in clipped_bias_dict[bits].values()]).cpu().numpy()\n",
        "\n",
        "    # Plot weight distribution for this bit width\n",
        "    all_weights = torch.cat([v.flatten() for v in clipped_weight_dict[bits].values()]).cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(all_biases, bins=50)\n",
        "    plt.title(f'Clipping Bias Distribution ({bits}-bit)')\n",
        "    plt.xlabel('Bias Value')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(all_weights, bins=100)\n",
        "    plt.title(f'Quantized Weight Distribution ({bits}-bit)')\n",
        "    plt.xlabel('Weight Value')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jEcmZ-pGz6lQ"
      },
      "id": "jEcmZ-pGz6lQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for bits in clipped_bias_dict_U.keys():\n",
        "    # Plot bias distribution for this bit width\n",
        "    all_biases = torch.cat([v.flatten() for v in clipped_bias_dict_U[bits].values()]).cpu().numpy()\n",
        "\n",
        "    # Plot weight distribution for this bit width\n",
        "    all_weights = torch.cat([v.flatten() for v in clipped_weight_dict_U[bits].values()]).cpu().numpy()\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.hist(all_biases, bins=50)\n",
        "    plt.title(f'Clipping Bias Distribution ({bits}-bit)')\n",
        "    plt.xlabel('Bias Value')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.hist(all_weights, bins=100)\n",
        "    plt.title(f'Quantized Weight Distribution ({bits}-bit)')\n",
        "    plt.xlabel('Weight Value')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "db8ISq_c2rhp"
      },
      "id": "db8ISq_c2rhp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d909489",
      "metadata": {
        "id": "6d909489"
      },
      "outputs": [],
      "source": [
        "activation_names = list(activations_dict[16].keys())\n",
        "num_bits = len(bit_array)\n",
        "num_activations = len(activation_names)\n",
        "\n",
        "fig, axes = plt.subplots(num_bits, num_activations, figsize=(20, 25))\n",
        "\n",
        "for i, bits in enumerate(bit_array):\n",
        "  for j, name in enumerate(activation_names):\n",
        "    ax = axes[i, j]\n",
        "\n",
        "    act_values = activations_dict[bits][name].cpu().flatten()\n",
        "\n",
        "    ax.hist(act_values, bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax.set_title(f'{bits}-bit: {name}')\n",
        "    ax.set_xlabel('Activation Value')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd73d1f9",
      "metadata": {
        "id": "dd73d1f9"
      },
      "outputs": [],
      "source": [
        "activation_names = list(activations_dict2_0[16].keys())\n",
        "num_bits = len(bit_array)\n",
        "num_activations = len(activation_names)\n",
        "\n",
        "fig, axes = plt.subplots(num_bits, num_activations, figsize=(20, 25))\n",
        "\n",
        "for i, bits in enumerate(bit_array):\n",
        "  for j, name in enumerate(activation_names):\n",
        "    ax = axes[i, j]\n",
        "\n",
        "    act_values = activations_dict2_0[bits][name].cpu().flatten()\n",
        "\n",
        "    ax.hist(act_values, bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax.set_title(f'{bits}-bit AdderNet2.0: {name}')\n",
        "    ax.set_xlabel('Activation Value')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "activation_names = list(activations_dict2_0_U[16].keys())\n",
        "num_bits = len(bit_array)\n",
        "num_activations = len(activation_names)\n",
        "\n",
        "fig, axes = plt.subplots(num_bits, num_activations, figsize=(20, 25))\n",
        "\n",
        "for i, bits in enumerate(bit_array):\n",
        "  for j, name in enumerate(activation_names):\n",
        "    ax = axes[i, j]\n",
        "\n",
        "    act_values = activations_dict2_0_U[bits][name].cpu().flatten()\n",
        "\n",
        "    ax.hist(act_values, bins=50, edgecolor='black', alpha=0.7)\n",
        "    ax.set_title(f'{bits}-bit AdderNet2.0: {name}')\n",
        "    ax.set_xlabel('Activation Value')\n",
        "    ax.set_ylabel('Frequency')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GjFVE45o5wR8"
      },
      "id": "GjFVE45o5wR8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6668ee47",
      "metadata": {
        "id": "6668ee47"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.scatter(bit_array, quant_acc, s=80, c='green', alpha=0.7, label='AdderNet (Original)')\n",
        "ax.plot(bit_array, quant_acc, linewidth=2, c='green', linestyle='--')\n",
        "ax.scatter(bit_array, quant_acc_2_0, s=80, c='blue', alpha=0.7, label='AdderNet2.0')\n",
        "ax.plot(bit_array, quant_acc_2_0, linewidth=2, c='r')\n",
        "ax.scatter(bit_array, quant_acc_2_0_U, s=80, c='red', alpha=0.7, label='AdderNet2.0 Unsigned')\n",
        "ax.plot(bit_array, quant_acc_2_0_U, linewidth=2, c='r')\n",
        "\n",
        "ax.set_title('Bitwidth vs. Accuracy')\n",
        "ax.set_xlabel('BitWidth')\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True)\n",
        "ax.invert_xaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a789b082",
      "metadata": {
        "id": "a789b082"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "ax.scatter(bit_array, quant_error, s=80, c='green', alpha=0.7, label='Testing Datapoints')\n",
        "ax.plot(bit_array, quant_error, linewidth=2, c='r')\n",
        "ax.set_title('Bitwidth vs. Quantization Error')\n",
        "ax.set_xlabel('BitWidth')\n",
        "ax.set_ylabel('Error')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}